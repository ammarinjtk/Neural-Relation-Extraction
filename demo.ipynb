{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from imports import *\n",
    "from utils import *\n",
    "from constants import *\n",
    "from models import *\n",
    "from trains import train_model\n",
    "from predicts import predict_model\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(239387)\n",
    "torch.manual_seed(239387)\n",
    "random.seed(239387)\n",
    "np.random.seed(239387)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "\n",
    "with open(\"./data/preprocessed_BB/train.json\", \"r\") as read_file:\n",
    "    dataloaders['train'] = json.load(read_file)\n",
    "    \n",
    "with open(\"./data/preprocessed_BB/dev.json\", \"r\") as read_file:\n",
    "    dataloaders['dev'] = json.load(read_file)\n",
    "    \n",
    "with open(\"./data/preprocessed_BB/test.json\", \"r\") as read_file:\n",
    "    dataloaders['test'] = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['train']), len(dataloaders['dev']), len(dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {'train': {}, 'dev': {}, 'test': {}}\n",
    "\n",
    "for phase in ['train', 'dev', 'test']:\n",
    "    for dataloader in dataloaders[phase]:\n",
    "        try:\n",
    "            label[phase][dataloader['label']]\n",
    "        except KeyError:\n",
    "            label[phase][dataloader['label']] = 0\n",
    "        else:\n",
    "            label[phase][dataloader['label']] += 1\n",
    "            \n",
    "print(\"Data statistic:\")\n",
    "print(f\"  - Train: {label['train']}\")\n",
    "print(f\"  - Dev: {label['dev']}\")\n",
    "print(f\"  - Test: {label['test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab\n",
    "vocabs = {}\n",
    "for phase in ['train', 'dev', 'test']:\n",
    "    for dataloader in dataloaders[phase]:\n",
    "        for word in dataloader['full_inputs']['full_token']:\n",
    "            try:\n",
    "                vocabs[word]\n",
    "            except KeyError:\n",
    "                vocabs[word] = 1\n",
    "            else:\n",
    "                vocabs[word] += 1\n",
    "\n",
    "# Sort by freq\n",
    "vocabs = sorted(vocabs.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load w2v model\n",
    "w2v_model = word2vec.KeyedVectors.load_word2vec_format(W2V_MODEL_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Global max relative distance\n",
    "max_distance = float(np.max([np.max(np.abs(input_dict['full_inputs']['full_dist1'] + input_dict['full_inputs']['full_dist2'])) for input_dict in dataloaders['train']+dataloaders['dev']+dataloaders['test']]))\n",
    "\n",
    "word_to_ix, pos_to_ix, distance_to_ix, dependency_to_ix, char_to_ix, in_vocab_count = build_vocab(dataloaders, w2v_model, vocabs)\n",
    "\n",
    "pretrained_embedding_matrix, distance_pretrain_embedding_matrix = build_pretrain_embedding_matrix(w2v_model, \n",
    "                                                                                                  word_to_ix, \n",
    "                                                                                                  distance_to_ix, \n",
    "                                                                                                  max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_max_sentence_length = np.max([np.max(\n",
    "                [len(input_dict['full_inputs']['full_token']), \n",
    "                 len(input_dict['full_inputs']['full_pos']), \n",
    "                 len(input_dict['full_inputs']['full_dist1']),\n",
    "                 len(input_dict['full_inputs']['full_dist2'])]) for input_dict in dataloaders['train']+dataloaders['dev']+dataloaders['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "model = Frankenstein(len(word_to_ix), len(pos_to_ix), len(distance_to_ix),\n",
    "                     glob_max_sentence_length, pretrained_embedding_matrix, distance_pretrain_embedding_matrix, \n",
    "                     batch_size, drop=0.5, hidden_dim=64, h=2, multihead_sizes=3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ELMo and BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELMo\n",
    "elmo_model = Elmo(ELMO_OPTIONS_PATH, ELMO_MODEL_PATH, 1, dropout=0)\n",
    "\n",
    "# BERT\n",
    "finetune_berts = []\n",
    "all_dataloaders = dataloaders['train']+dataloaders['dev']+dataloaders['test']\n",
    "with open(BERT_FEATURES_PATH, 'rb') as f: # opening file in binary(rb) mode    \n",
    "    for idx, item in enumerate(json_lines.reader(f)):\n",
    "        all_dataloaders[idx]['bert_features'] = np.sum([np.array(layer['values']) for layer in item['features'][0]['layers']], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10 epochs\n",
    "\n",
    "train_out = train_model(model, elmo_model, dataloaders['train'], dataloaders['dev'], word_to_ix, pos_to_ix, \n",
    "                        distance_to_ix, criterion, optimizer_ft, num_epochs=100, \n",
    "                        early_stopped_patience=10, batch_size=batch_size)\n",
    "\n",
    "(model, train_f1, val_f1, history) = train_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Generating submission (.a2) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, test_dataloader = predict_model(model, elmo_model, dataloaders['test'], word_to_ix, \n",
    "                                            pos_to_ix, distance_to_ix, batch_size, optimizer_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred), len(dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output path\n",
    "if not os.path.exists(OUTPUT_DIR_PATH):\n",
    "    os.mkdir(OUTPUT_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_name = \"test_prediction\"\n",
    "\n",
    "if not os.path.exists(f'{OUTPUT_DIR_PATH}/{model_dir_name}'):\n",
    "    os.mkdir(f'{OUTPUT_DIR_PATH}/{model_dir_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = minidom.parse(\"./data/BioNLP-ST-2016_BB-event_test.xml\")\n",
    "docs = file.getElementsByTagName(\"document\")\n",
    "all_test_files = []\n",
    "for doc in docs:\n",
    "    all_test_files.append(doc.getAttribute(\"origId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict = {}\n",
    "relation_idx_dict = {}\n",
    "pred_test_files = set()\n",
    "for idx, input_dict in enumerate(test_data):\n",
    "    inputs = input_dict['shortest_inputs']\n",
    "    entity_tag = input_dict['entity_pair']\n",
    "    label = input_dict['label']\n",
    "    entity_idx_to_type = input_dict['entity_idx_to_type']\n",
    "\n",
    "    if y_pred[idx] == 1:\n",
    "        document_idx = input_dict['document_id']\n",
    "        pred_test_files.add(document_idx)\n",
    "        entity_idx_to_origId = input_dict['entity_idx_to_origId']\n",
    "        first_match = re.match(r'(BB-event-\\d+).(T\\d+)', entity_idx_to_origId[entity_tag[0]])\n",
    "        second_match = re.match(r'(BB-event-\\d+).(T\\d+)', entity_idx_to_origId[entity_tag[1]])\n",
    "        \n",
    "        first_entity = first_match.group(2).upper()\n",
    "        second_entity = second_match.group(2).upper()\n",
    "        first_doc = first_match.group(1)\n",
    "        second_doc = second_match.group(1)\n",
    "        \n",
    "        try:\n",
    "            relation_idx_dict[document_idx] += 1\n",
    "        except KeyError:\n",
    "            relation_idx_dict[document_idx] = 1\n",
    "        \n",
    "        try:\n",
    "            write_dict[f\"{OUTPUT_DIR_PATH}/{model_dir_name}/{document_idx}.a2\"]\n",
    "        except KeyError:\n",
    "            write_dict[f\"{OUTPUT_DIR_PATH}/{model_dir_name}/{document_idx}.a2\"] = set()\n",
    "        write_dict[f\"{OUTPUT_DIR_PATH}/{model_dir_name}/{document_idx}.a2\"].add(f\"R{relation_idx_dict[document_idx]}\\tLives_In Bacteria:{first_entity} Location:{second_entity}\\n\")\n",
    "        \n",
    "for key, value in write_dict.items():\n",
    "    write_str = \"\".join([i[1] for i in sorted([(int(d.split('\\t')[0][1:]), d) for d in list(value)], key=lambda tup: tup[0])])\n",
    "    f = open(f\"{key}\", \"w\")\n",
    "    f.write(write_str)\n",
    "    f.close()   \n",
    "    \n",
    "pred_test_files = list(pred_test_files)\n",
    "for test_file in all_test_files:\n",
    "    if not test_file in pred_test_files:\n",
    "        f = open(f\"{OUTPUT_DIR_PATH}/{model_dir_name}/{test_file}.a2\", \"a+\")\n",
    "        f.write(\"\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission .zip file \n",
    "shutil.make_archive(f\"{OUTPUT_DIR_PATH}/{model_dir_name}\", 'zip', f\"{OUTPUT_DIR_PATH}/{model_dir_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove submission folders\n",
    "shutil.rmtree(f\"{OUTPUT_DIR_PATH}/{model_dir_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
