{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from imports import *\n",
    "from utils import *\n",
    "from constants import *\n",
    "from models import *\n",
    "from trains import train_model\n",
    "from predicts import predict_model\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/hdd/ammarinjtk/OntoBiotope_BioNLP-ST-2016.obo\") as file:  \n",
    "#     data = [x.split(\"\\n\") for x in file.read().split(\"\\n\\n\")[2:]]\n",
    "# synonym_pattern = r\"synonym: \\\"([\\w\\s]+)\\\" EXACT\"\n",
    "# name_pattern = r\"name: ([\\w\\s]+)\"\n",
    "# is_a_pattern = r\"is_a: [\\w\\s\\d:]+ ! ([\\w ]+)\"\n",
    "\n",
    "# ontologies = []\n",
    "\n",
    "# for x in data:\n",
    "#     ontology_dict = {\n",
    "#         'name': '',\n",
    "#         'synonym': [],\n",
    "#         'is_a': ''\n",
    "#     }\n",
    "\n",
    "#     for y in x:\n",
    "\n",
    "#         if re.match(name_pattern, y):\n",
    "#             ontology_dict['name'] = re.match(name_pattern, y).group(1)\n",
    "#         elif re.match(synonym_pattern, y):\n",
    "#             ontology_dict['synonym'].append(re.match(synonym_pattern, y).group(1))\n",
    "#         elif re.match(is_a_pattern, y):\n",
    "#             ontology_dict['is_a'] = re.match(is_a_pattern, y).group(1)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#     ontologies.append(ontology_dict)\n",
    "# synonym_dict = {}\n",
    "# for x in [x for x in ontologies if len(x['synonym']) > 0]:\n",
    "#     for synonym in x['synonym']:\n",
    "#         synonym_dict[synonym.lower()] = x['name'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataloaders = preprocess(\"/home/ammarinjtk/pytorch/Corpus_BB3/\", synonym_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non-Deterministic behaviour\n",
    "# for key, dataloader in dataloaders.items():\n",
    "#     random.shuffle(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/original_data/train.json', 'w') as outfile:  \n",
    "#     json.dump(dataloaders['train'], outfile)\n",
    "    \n",
    "# with open('./data/original_data/dev.json', 'w') as outfile:  \n",
    "#     json.dump(dataloaders['dev'], outfile)\n",
    "    \n",
    "# with open('./data/original_data/test.json', 'w') as outfile:  \n",
    "#     json.dump(dataloaders['test'], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "\n",
    "with open(\"./data/preprocessed_BB/train.json\", \"r\") as read_file:\n",
    "    dataloaders['train'] = json.load(read_file)\n",
    "    \n",
    "with open(\"./data/preprocessed_BB/dev.json\", \"r\") as read_file:\n",
    "    dataloaders['dev'] = json.load(read_file)\n",
    "    \n",
    "with open(\"./data/preprocessed_BB/test.json\", \"r\") as read_file:\n",
    "    dataloaders['test'] = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['train']), len(dataloaders['dev']), len(dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load w2v model\n",
    "w2v_model = word2vec.KeyedVectors.load_word2vec_format('/hdd/ammarinjtk/wikipedia-pubmed-and-PMC-w2v.bin', binary=True)\n",
    "# w2v_model = gensim.models.Word2Vec.load(\"/hdd/ammarinjtk/li_reimplement/models/5_epochs.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Global max relative distance\n",
    "max_distance = float(np.max([np.max(np.abs(input_dict['full_inputs']['full_dist1'] + input_dict['full_inputs']['full_dist2'])) for input_dict in dataloaders['train']+dataloaders['dev']+dataloaders['test']]))\n",
    "\n",
    "word_to_ix, pos_to_ix, distance_to_ix, dependency_to_ix, char_to_ix, in_vocab_count = build_vocab(dataloaders, w2v_model)\n",
    "\n",
    "pretrained_embedding_matrix, distance_pretrain_embedding_matrix = build_pretrain_embedding_matrix(w2v_model, \n",
    "                                                                                                  word_to_ix, \n",
    "                                                                                                  distance_to_ix, \n",
    "                                                                                                  max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_shortest_max_sentence_length = np.max([np.max(\n",
    "                [len(input_dict['shortest_inputs']['shortest_token']), \n",
    "                 len(input_dict['shortest_inputs']['shortest_pos']), \n",
    "                 len(input_dict['shortest_inputs']['shortest_dep'])]) for input_dict in dataloaders['train']+dataloaders['dev']+dataloaders['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_max_sentence_length = np.max([np.max(\n",
    "                [len(input_dict['full_inputs']['full_token']), \n",
    "                 len(input_dict['full_inputs']['full_pos']), \n",
    "                 len(input_dict['full_inputs']['full_dep'])]) for input_dict in dataloaders['train']+dataloaders['dev']+dataloaders['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "torch.cuda.manual_seed_all(523454)\n",
    "torch.manual_seed(523454)\n",
    "random.seed(523454)\n",
    "np.random.seed(523454)\n",
    "\n",
    "model = Frankenstein(len(word_to_ix), len(pos_to_ix), len(distance_to_ix), len(dependency_to_ix),\n",
    "                     glob_max_sentence_length, pretrained_embedding_matrix, distance_pretrain_embedding_matrix, \n",
    "                     batch_size, drop=0.5, wdrop=0.3, edrop=0.3, idrop=0.3, hidden_dim=64, \n",
    "                     window_sizes=[3, 5, 7], h=1, multihead_sizes=3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=1e-3, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ELMo and BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "\n",
    "options_file = \"/hdd/ammarinjtk/ELMO_model/revised_bacteria_pubmed/options.json\"\n",
    "weight_file = \"/hdd/ammarinjtk/ELMO_model/revised_bacteria_pubmed/headentity_finetune_weights.hdf5\"\n",
    "\n",
    "elmo_model = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "\n",
    "finetune_berts = []\n",
    "with open('/hdd/ammarinjtk/BERT_features/synonym_revised_headentity/finetune_bert.jsonl', 'rb') as f: # opening file in binary(rb) mode    \n",
    "    for item in json_lines.reader(f):\n",
    "        finetune_berts.append(item)\n",
    "\n",
    "with open('/hdd/ammarinjtk/BERT_features/synonym_revised_headentity/tkn.txt', 'r') as f: \n",
    "    full_bert_tkns = f.read()\n",
    "    \n",
    "dataloader_count = 0\n",
    "for idx, full_bert_tkn in enumerate(full_bert_tkns.split('\\n')):\n",
    "    \n",
    "    for dataloader in dataloaders['train']+dataloaders['dev']+dataloaders['test']:\n",
    "        if \" \".join(dataloader['full_inputs']['full_token']) == full_bert_tkn:\n",
    "            dataloader_count += 1\n",
    "            dataloader['bert_features'] = np.sum([np.array(layer['values']) for layer in finetune_berts[0]['features'][0]['layers']], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_out = train_model(model, elmo_model, dataloaders['train'], dataloaders['dev'], word_to_ix, pos_to_ix, \n",
    "                        distance_to_ix, dependency_to_ix, criterion, optimizer_ft, lr_scheduler=None, \n",
    "                        num_epochs=5, early_stopped_patience=1, batch_size=batch_size)\n",
    "\n",
    "(model, train_f1, val_f1, history) = train_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, predictions, self_attn_scores, ent_attn_scores, multihead_attn_scores = predict_model(model, elmo_model, \n",
    "                                  dataloaders['test'], word_to_ix, pos_to_ix, distance_to_ix,\n",
    "                                  dependency_to_ix, char_to_ix, batch_size, optimizer_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(y_pred), len(dataloaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction (.a2) file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dir_name = \"test_prediction\"\n",
    "os.mkdir(f'/hdd/ammarinjtk/{model_dir_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataloaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = minidom.parse(\"/home/ammarinjtk/pytorch/Corpus_BB3/BioNLP-ST-2016_BB-event_{}.xml\".format('test'))\n",
    "docs = file.getElementsByTagName(\"document\")\n",
    "all_test_files = []\n",
    "for doc in docs:\n",
    "    all_test_files.append(doc.getAttribute(\"origId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict = {}\n",
    "relation_idx_dict = {}\n",
    "pred_test_files = set()\n",
    "for idx, input_dict in enumerate(test_data):\n",
    "    inputs = input_dict['shortest_inputs']\n",
    "    entity_tag = input_dict['entity_pair']\n",
    "    label = input_dict['label']\n",
    "    entity_idx_to_type = input_dict['entity_idx_to_type']\n",
    "\n",
    "    if y_pred[idx] == 1:\n",
    "        document_idx = input_dict['document_id']\n",
    "        pred_test_files.add(document_idx)\n",
    "        entity_idx_to_origId = input_dict['entity_idx_to_origId']\n",
    "        first_match = re.match(r'(BB-event-\\d+).(T\\d+)', entity_idx_to_origId[entity_tag[0]])\n",
    "        second_match = re.match(r'(BB-event-\\d+).(T\\d+)', entity_idx_to_origId[entity_tag[1]])\n",
    "        \n",
    "        first_entity = first_match.group(2).upper()\n",
    "        second_entity = second_match.group(2).upper()\n",
    "        first_doc = first_match.group(1)\n",
    "        second_doc = second_match.group(1)\n",
    "        \n",
    "        try:\n",
    "            relation_idx_dict[document_idx] += 1\n",
    "        except KeyError:\n",
    "            relation_idx_dict[document_idx] = 1\n",
    "        \n",
    "        try:\n",
    "            write_dict[\"/hdd/ammarinjtk/{}/{}.a2\".format(model_dir_name, document_idx)]\n",
    "        except KeyError:\n",
    "            write_dict[\"/hdd/ammarinjtk/{}/{}.a2\".format(model_dir_name, document_idx)] = set()\n",
    "        write_dict[\"/hdd/ammarinjtk/{}/{}.a2\".format(model_dir_name, document_idx)].add(\"R{}\\tLives_In Bacteria:{} Location:{}\\n\".format(relation_idx_dict[document_idx], first_entity, second_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, value in write_dict.items():\n",
    "    \n",
    "    write_str = \"\".join([i[1] for i in sorted([(int(d.split('\\t')[0][1:]), d) for d in list(value)], key=lambda tup: tup[0])])\n",
    "    \n",
    "    f = open(f\"{key}\", \"w\")\n",
    "    f.write(write_str)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_test_files), len(pred_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_files = list(pred_test_files)\n",
    "for test_file in all_test_files:\n",
    "    if not test_file in pred_test_files:\n",
    "        print(test_file)\n",
    "        f = open(\"/hdd/ammarinjtk/{}/{}.a2\".format(model_dir_name, test_file), \"a+\")\n",
    "        f.write(\"\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "len(glob.glob(f\"/hdd/ammarinjtk/{model_dir_name}/*.a2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(f\"/hdd/ammarinjtk/{model_dir_name}\",\n",
    "                    'zip',\n",
    "                    f\"/hdd/ammarinjtk/{model_dir_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
